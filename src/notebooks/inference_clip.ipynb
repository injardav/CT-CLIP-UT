{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a83a64-68b9-4863-8898-fd6d9fde29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f29a03-6f8e-4f6f-b70d-cb39422517c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Successfully loaded state dictionary from: /project/project_465001111/ct_clip/pretrained_models/ctclip_v2.pt\n",
      "Validation size: 1\n",
      "Evaluation started\n",
      "occlusion visualization started.\n",
      "Repeat scores for same patch: [1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625, 1.25390625]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "import torch\n",
    "import utils.CTClipInference\n",
    "from monai.utils import ensure_tuple_rep\n",
    "from models.ctclip import CTCLIP\n",
    "from utils.ctvit import CTViT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers.utils import logging\n",
    "from torch import nn\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.set_verbosity_error()\n",
    "torch.set_printoptions(profile=\"default\")\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\")\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dim_latent = 512\n",
    "dim_text = 768\n",
    "vit_dim_image = 294912\n",
    "\n",
    "vit_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "clip = CTCLIP(\n",
    "    text_encoder = text_encoder,\n",
    "    image_encoder = vit_encoder,\n",
    "    dim_text = dim_text,\n",
    "    dim_image = vit_dim_image,\n",
    "    dim_latent = dim_latent\n",
    ")\n",
    "\n",
    "clip.load(\"/project/project_465001111/ct_clip/pretrained_models/ctclip_v2.pt\")\n",
    "\n",
    "inference = utils.CTClipInference.CTClipInference(\n",
    "    clip,\n",
    "    valid_reports = \"/project/project_465001111/ct_clip/CT-CLIP-UT/reports/valid_reports.csv\",\n",
    "    data_valid = \"/scratch/project_465001111/ct_clip/data_volumes/dataset/valid\",\n",
    "    valid_labels = \"/project/project_465001111/ct_clip/CT-CLIP-UT/labels/valid_labels.csv\",\n",
    "    valid_metadata = \"/project/project_465001111/ct_clip/CT-CLIP-UT/metadata/valid_metadata.csv\",\n",
    "    results_folder = \"/project/project_465001111/ct_clip/CT-CLIP-UT/src/results/valid/ctclip\",\n",
    "    tokenizer = tokenizer,\n",
    "    batch_size = 1,\n",
    "    num_workers = 4,\n",
    "    num_valid_samples = 1,\n",
    "    zero_shot = False,\n",
    "    visualize = True\n",
    ")\n",
    "\n",
    "inference.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f109f2-0b6c-4883-aeb0-6f84a7df1dca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
