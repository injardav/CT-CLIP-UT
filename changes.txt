diff --git a/CT_CLIP/ct_clip/ct_clip.py b/CT_CLIP/ct_clip/ct_clip.py
index 137e47a..1354f86 100644
--- a/CT_CLIP/ct_clip/ct_clip.py
+++ b/CT_CLIP/ct_clip/ct_clip.py
@@ -205,6 +205,8 @@ class Attention(nn.Module):
         self.to_out = nn.Sequential(nn.Linear(inner_dim, dim, bias = False), LayerNorm(dim))
         self.dropout = nn.Dropout(dropout)
 
+        self.attention_weights = None
+
     def forward(self, x, mask = None, rotary_pos_emb = None):
         h, device, scale = self.heads, x.device, self.scale
 
@@ -233,6 +235,8 @@ class Attention(nn.Module):
         attn = sim.softmax(dim = -1, dtype = torch.float32)
         attn = attn.type(sim.dtype)
 
+        self.attention_weights = attn
+
         attn = self.dropout(attn)
 
         out = einsum('b h i j, b h j d -> b h i d', attn, v)
@@ -255,7 +259,7 @@ class Transformer(nn.Module):
     ):
         super().__init__()
         self.checkpoint_during_training = checkpoint_during_training
-
+        self.attention_weights_per_layer = []
         self.layers = nn.ModuleList([])
         for _ in range(depth):
             self.layers.append(nn.ModuleList([
@@ -619,6 +623,7 @@ class CTCLIP(nn.Module):
             return_loss = False,
             return_encodings = False,
             return_latents = False,
+            return_attention = False,       
             freeze_image_encoder = False,   # image encoder is not trained if this is set to True, proposed by LiT paper
             freeze_text_encoder = False,    # text encoder is not trained if this is set to True
             text_to_image = True,           # in the case the extra projection is turned on, would return different similarity values depending on modality directionality
@@ -682,8 +687,8 @@ class CTCLIP(nn.Module):
             text_args = (*text_args, text_mask)
 
 
-        text_embeddings = self.text_transformer(text.input_ids, attention_mask = text.attention_mask )
-        enc_text = text_embeddings[0]
+        text_embeddings = self.text_transformer(text.input_ids, attention_mask = text.attention_mask, output_attentions=True )
+        enc_text, text_attention_weights = text_embeddings.last_hidden_state, text_embeddings.attentions
 
         # depending on whether text is using causal mask, post process, moving eos token to the first position
 
@@ -712,7 +717,10 @@ class CTCLIP(nn.Module):
             freeze = freeze_image_encoder
         )"""
 
-        enc_image= self.visual_transformer(image, return_encoded_tokens=True)
+        if return_attention:
+            enc_image, spatial_attention_weights, temporal_attention_weights= self.visual_transformer(image, return_encoded_tokens=True, text_embeddings=text_embeddings, return_attention=return_attention)
+        else:
+            enc_image= self.visual_transformer(image, return_encoded_tokens=True)
 
         #print("This is visual encoding")
         global h_r, w_r, z_r
@@ -800,11 +808,17 @@ class CTCLIP(nn.Module):
 
         if not return_loss and self.use_all_token_embeds:
             einsum_args = (text_latents_extra, image_latents_extra) if self.extra_latent_projection and not text_to_image else (text_latents, image_latents)
-            return einsum('b d, b i d -> b t i', *einsum_args) * temp
+            result = einsum('b d, b i d -> b t i', *einsum_args) * temp
+            if return_attention:
+                return result, text_attention_weights, spatial_attention_weights, temporal_attention_weights
+            return result
 
         if not return_loss and not self.use_all_token_embeds:
             einsum_args = (text_latents_extra, image_latents_extra) if self.extra_latent_projection and not text_to_image else (text_latents, image_latents)
-            return einsum('b d, b d -> b', *einsum_args) * temp
+            result = einsum('b d, b d -> b', *einsum_args) * temp
+            if return_attention:
+                return result, text_attention_weights, spatial_attention_weights, temporal_attention_weights
+            return result
 
         # split out multiview dimension for text and images
 
diff --git a/scripts/run_zero_shot.py b/scripts/run_zero_shot.py
index d6ed6ba..6694e7b 100755
--- a/scripts/run_zero_shot.py
+++ b/scripts/run_zero_shot.py
@@ -1,51 +1,64 @@
 import torch
+import argparse
 from transformer_maskgit import CTViT
 from transformers import BertTokenizer, BertModel
 from ct_clip import CTCLIP
 from zero_shot import CTClipInference
 import accelerate
 
-tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)
-text_encoder = BertModel.from_pretrained("microsoft/BiomedVLP-CXR-BERT-specialized")
-
-text_encoder.resize_token_embeddings(len(tokenizer))
-
-
-image_encoder = CTViT(
-    dim = 512,
-    codebook_size = 8192,
-    image_size = 480,
-    patch_size = 20,
-    temporal_patch_size = 10,
-    spatial_depth = 4,
-    temporal_depth = 4,
-    dim_head = 32,
-    heads = 8
-)
-
-clip = CTCLIP(
-    image_encoder = image_encoder,
-    text_encoder = text_encoder,
-    dim_image = 294912,
-    dim_text = 768,
-    dim_latent = 512,
-    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)
-    use_mlm=False,
-    downsample_image_embeds = False,
-    use_all_token_embeds = False
-
-)
-
-clip.load("path_to_pretrained_model")
-
-inference = CTClipInference(
-    clip,
-    data_folder = 'path_to_preprocessed_validation_folder',
-    reports_file= "path_to_validation_reports_csv",
-    labels = "path_to_validation_labels_csv",
-    batch_size = 1,
-    results_folder="inference_zeroshot/",
-    num_train_steps = 1,
-)
-
-inference.infer()
+def main():
+    parser = argparse.ArgumentParser(description="Run CT-CLIP inference.")
+    parser.add_argument("--save_weights", action="store_true", help="Flag to save attention weights during inference.")
+    args = parser.parse_args()
+
+    tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)
+    text_encoder = BertModel.from_pretrained("microsoft/BiomedVLP-CXR-BERT-specialized")
+    text_encoder.resize_token_embeddings(len(tokenizer))
+
+
+    image_encoder = CTViT(
+        dim = 512,
+        codebook_size = 8192,
+        image_size = 480,
+        patch_size = 20,
+        temporal_patch_size = 10,
+        spatial_depth = 4,
+        temporal_depth = 4,
+        dim_head = 32,
+        heads = 8
+    )
+
+    clip = CTCLIP(
+        image_encoder = image_encoder,
+        text_encoder = text_encoder,
+        dim_image = 294912,
+        dim_text = 768,
+        dim_latent = 512,
+        extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)
+        use_mlm=False,
+        downsample_image_embeds = False,
+        use_all_token_embeds = False
+
+    )
+
+    clip.load("/mnt/ct_clip/models/CT-CLIP_v2.pt")
+
+    inference = CTClipInference(
+        clip,
+        data_folder = '/mnt/ct_clip_data/valid_preprocessed',
+        reports_file= "/mnt/ct_clip_data/reports/valid_reports.csv",
+        labels = "/mnt/ct_clip_data/labels/valid_labels.csv",
+        batch_size = 1,
+        results_folder="/mnt/ct_clip_home/inference_zeroshot/",
+        num_train_steps = 1,
+        save_weights = args.save_weights,
+        single_sample = args.single_sample
+    )
+
+    print(f"Script arg 'save_weights': {args.save_weights}")
+    print("Running inference...")
+    inference.infer()
+    print("Inference completed. Results saved to: /mnt/ct_clip_data/inference_zeroshot/")
+
+if __name__ == "__main__":
+    main()
diff --git a/scripts/zero_shot.py b/scripts/zero_shot.py
index 2107976..86efcf4 100644
--- a/scripts/zero_shot.py
+++ b/scripts/zero_shot.py
@@ -12,7 +12,7 @@ from torch import nn
 from torch.utils.data import Dataset, DataLoader, random_split
 from torch.utils.data.distributed import DistributedSampler
 
-from data_inference_nii import CTReportDatasetinfer
+from data_inference import CTReportDatasetinfer
 #from data_external_valid import CTReportDatasetinfer
 import numpy as np
 import tqdm
@@ -150,12 +150,14 @@ class CTClipInference(nn.Module):
         save_model_every = 2000,
         results_folder = './results',
         labels = "labels.csv",
+        save_weights = False,
         accelerate_kwargs: dict = dict()
     ):
         super().__init__()
         ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
         self.accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], **accelerate_kwargs)
         self.CTClip = CTClip
+        self.save_weights = save_weights
         self.tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)
         self.results_folder = results_folder
         self.register_buffer('steps', torch.Tensor([0]))
@@ -265,6 +267,7 @@ class CTClipInference(nn.Module):
                     text_latent_list = []
                     image_latent_list = []
                     accession_names=[]
+                    attention_weights_all = []
                     pathologies = ['Medical material','Arterial wall calcification', 'Cardiomegaly', 'Pericardial effusion','Coronary artery wall calcification', 'Hiatal hernia','Lymphadenopathy', 'Emphysema', 'Atelectasis', 'Lung nodule','Lung opacity', 'Pulmonary fibrotic sequela', 'Pleural effusion', 'Mosaic attenuation pattern','Peribronchial thickening', 'Consolidation', 'Bronchiectasis','Interlobular septal thickening']
                     for i in tqdm.tqdm(range(len(self.ds))):
                         valid_data, text, onehotlabels, acc_name = next(self.dl_iter)
@@ -280,7 +283,19 @@ class CTClipInference(nn.Module):
                             text_tokens=self.tokenizer(
                                             text, return_tensors="pt", padding="max_length", truncation=True, max_length=512).to(device)
 
-                            output = model(text_tokens, valid_data.cuda(),  device=device)
+                            if self.save_weights:
+                                output, text_attention_weights, spatial_attention_weights, temporal_attention_weights = model(text_tokens, valid_data.cuda(), device=device, return_attention=self.save_weights)
+                                np.savez(
+                                    f"{plotdir}attention_weights_{i}.npz",
+                                    text_attention_weights=[attn.cpu().numpy() for attn in text_attention_weights],  # Handle text attention weights
+                                    spatial_self_attention_weights=[attn.cpu().numpy() for attn in spatial_attention_weights[0]],  # Self-attention for spatial
+                                    spatial_cross_attention_weights=[attn.cpu().numpy() for attn in spatial_attention_weights[1]],  # Cross-attention for spatial
+                                    temporal_self_attention_weights=[attn.cpu().numpy() for attn in temporal_attention_weights[0]],  # Self-attention for temporal
+                                    temporal_cross_attention_weights=[attn.cpu().numpy() for attn in temporal_attention_weights[1]],  # Cross-attention for temporal
+                                    accession=acc_name[0]
+                                )
+                            else:
+                                output = model(text_tokens, valid_data.cuda(), device=device)
 
                             output = apply_softmax(output)
 
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/MaskGITTransformer.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/MaskGITTransformer.cpython-310.pyc
index 3498994..cb25ff7 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/MaskGITTransformer.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/MaskGITTransformer.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/__init__.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/__init__.cpython-310.pyc
index fe0ae86..1738ba2 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/__init__.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/__init__.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/attention.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/attention.cpython-310.pyc
index 9d2482e..224d333 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/attention.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/attention.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/ctvit.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/ctvit.cpython-310.pyc
index 12cb807..30e3e55 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/ctvit.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/ctvit.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/ctvit_trainer.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/ctvit_trainer.cpython-310.pyc
index 55f3b44..bd36692 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/ctvit_trainer.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/ctvit_trainer.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/data.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/data.cpython-310.pyc
index 740dde2..99bffff 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/data.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/data.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/optimizer.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/optimizer.cpython-310.pyc
index efba039..dd318b9 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/optimizer.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/optimizer.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/t5.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/t5.cpython-310.pyc
index 84f40cc..036b88e 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/t5.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/t5.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/__pycache__/videotextdataset.cpython-310.pyc b/transformer_maskgit/transformer_maskgit/__pycache__/videotextdataset.cpython-310.pyc
index 95be43b..584af52 100644
Binary files a/transformer_maskgit/transformer_maskgit/__pycache__/videotextdataset.cpython-310.pyc and b/transformer_maskgit/transformer_maskgit/__pycache__/videotextdataset.cpython-310.pyc differ
diff --git a/transformer_maskgit/transformer_maskgit/attention.py b/transformer_maskgit/transformer_maskgit/attention.py
index 5cf8f46..9975a28 100644
--- a/transformer_maskgit/transformer_maskgit/attention.py
+++ b/transformer_maskgit/transformer_maskgit/attention.py
@@ -124,12 +124,15 @@ class Attention(nn.Module):
 
         self.to_out = nn.Linear(inner_dim, dim, bias = False)
 
+        self.attention_weights = None
+
     def forward(
         self,
         x,
         mask = None,
         context = None,
-        attn_bias = None
+        attn_bias = None,
+        return_attention = False
     ):
         batch, device, dtype = x.shape[0], x.device, x.dtype
         device=torch.device('cuda')
@@ -175,9 +178,14 @@ class Attention(nn.Module):
         attn = sim.softmax(dim = -1)
         attn = self.attn_dropout(attn)
 
+        self.attention_weights = attn
+
         out = einsum('b h i j, b h j d -> b h i d', attn, v)
 
         out = rearrange(out, 'b h n d -> b n (h d)')
+
+        if return_attention:
+            return self.to_out(out), self.attention_weights
         return self.to_out(out)
 
 # alibi positional bias for extrapolation
@@ -316,18 +324,34 @@ class Transformer(nn.Module):
         attn_bias = None,
         context = None,
         self_attn_mask = None,
-        cross_attn_context_mask = None
+        cross_attn_context_mask = None,
+        return_attention = False
     ):
+        self_attention_weights_list = []
+        cross_attention_weights_list = []
 
         for peg, self_attn, cross_attn, ff in self.layers:
             if exists(peg):
                 x = peg(x, shape = video_shape) + x
 
-            x = self_attn(x, attn_bias = attn_bias, mask = self_attn_mask) + x
+            if return_attention:
+                x_out, self_attn_weights  = self_attn(x, attn_bias = attn_bias, mask = self_attn_mask, return_attention=return_attention)
+                self_attention_weights_list.append(self_attn_weights )
+                x = x_out + x
+            else:
+                x = self_attn(x, attn_bias = attn_bias, mask = self_attn_mask) + x
 
             if exists(cross_attn) and exists(context):
-                x = cross_attn(x, context = context, mask = cross_attn_context_mask) + x
+                if return_attention:
+                    x_out, cross_attn_weights = cross_attn(x, context=context, mask=cross_attn_context_mask, return_attention=return_attention)
+                    cross_attention_weights_list.append(cross_attn_weights)
+                    x = x_out + x
+                else:
+                    x = cross_attn(x, context = context, mask = cross_attn_context_mask) + x
 
             x = ff(x) + x
 
+        if return_attention:
+            return self.norm_out(x), (self_attention_weights_list, cross_attention_weights_list)
+        
         return self.norm_out(x)
diff --git a/transformer_maskgit/transformer_maskgit/ctvit.py b/transformer_maskgit/transformer_maskgit/ctvit.py
index 44cd27c..da89683 100644
--- a/transformer_maskgit/transformer_maskgit/ctvit.py
+++ b/transformer_maskgit/transformer_maskgit/ctvit.py
@@ -182,6 +182,7 @@ class CTViT(nn.Module):
             ff_dropout = ff_dropout,
             peg = True,
             peg_causal = True,
+            has_cross_attn = True
         )
         self.enc_spatial_transformer = Transformer(depth = spatial_depth, **transformer_kwargs)
         self.enc_temporal_transformer = Transformer(depth = temporal_depth, **transformer_kwargs)
@@ -281,7 +282,9 @@ class CTViT(nn.Module):
 
     def encode(
         self,
-        tokens
+        tokens,
+        text_embeddings,
+        return_attention=False
     ):
         b = tokens.shape[0]
         h, w = self.patch_height_width
@@ -292,7 +295,10 @@ class CTViT(nn.Module):
         device=torch.device('cuda')
         attn_bias = self.spatial_rel_pos_bias(h, w, device = device)
 
-        tokens = self.enc_spatial_transformer(tokens, attn_bias = attn_bias, video_shape = video_shape)
+        if return_attention:
+            tokens, spatial_attention_weights = self.enc_spatial_transformer(tokens, attn_bias = attn_bias, video_shape = video_shape, context = text_embeddings, return_attention = return_attention)
+        else:
+            tokens = self.enc_spatial_transformer(tokens, attn_bias = attn_bias, video_shape = video_shape)
 
         tokens = rearrange(tokens, '(b t) (h w) d -> b t h w d', b = b, h = h , w = w)
 
@@ -300,10 +306,15 @@ class CTViT(nn.Module):
 
         tokens = rearrange(tokens, 'b t h w d -> (b h w) t d')
 
-        tokens = self.enc_temporal_transformer(tokens, video_shape = video_shape)
+        if return_attention:
+            tokens, temporal_attention_weights = self.enc_temporal_transformer(tokens, video_shape = video_shape, context = text_embeddings, return_attention = return_attention)
+        else:
+            tokens = self.enc_temporal_transformer(tokens, video_shape = video_shape)
 
         tokens = rearrange(tokens, '(b h w) t d -> b t h w d', b = b, h = h, w = w)
 
+        if return_attention:
+            return tokens, spatial_attention_weights, temporal_attention_weights
         return tokens
 
     def decode(
@@ -359,7 +370,9 @@ class CTViT(nn.Module):
         return_discr_loss = False,
         apply_grad_penalty = True,
         return_only_codebook_ids = False,
-        return_encoded_tokens=False
+        return_encoded_tokens = False,
+        text_embeddings = None,
+        return_attention = False
     ):
         assert video.ndim in {4, 5}
 
@@ -390,7 +403,10 @@ class CTViT(nn.Module):
 
         # encode - spatial
 
-        tokens = self.encode(tokens)
+        if return_attention:
+            tokens, spatial_attention_weights, temporal_attention_weights = self.encode(tokens, text_embeddings, return_attention)
+        else:
+            tokens = self.encode(tokens)
 
         # quantize
 
@@ -409,6 +425,8 @@ class CTViT(nn.Module):
         tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h = h, w = w)
 
         if return_encoded_tokens:
+            if return_attention:
+                return tokens, spatial_attention_weights, temporal_attention_weights
             return tokens
             
         recon_video = self.decode(tokens)
