{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f526df41-d3f7-4a06-9aff-530623c18af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93cb4302-14ae-48f4-a529-240f60d43221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import nibabel as nib\n",
    "import math\n",
    "import torch\n",
    "import time\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgba, to_hex\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from transformers.utils import logging\n",
    "from IPython.display import display, HTML\n",
    "from torch.utils.data import RandomSampler, DataLoader\n",
    "from utils.InferenceDataset import InferenceDataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from models.ctclip import CTCLIP\n",
    "from utils.ctvit import CTViT\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "mpl.rcParams['animation.embed_limit'] = 128\n",
    "\n",
    "pathologies = [\n",
    "    \"Medical material\", \"Arterial wall calcification\", \"Cardiomegaly\",\n",
    "    \"Pericardial effusion\", \"Coronary artery wall calcification\", \"Hiatal hernia\",\n",
    "    \"Lymphadenopathy\", \"Emphysema\", \"Atelectasis\", \"Lung nodule\",\n",
    "    \"Lung opacity\", \"Pulmonary fibrotic sequela\", \"Pleural effusion\",\n",
    "    \"Mosaic attenuation pattern\", \"Peribronchial thickening\", \"Consolidation\",\n",
    "    \"Bronchiectasis\", \"Interlobular septal thickening\"\n",
    "]\n",
    "colors = [\n",
    "    \"red\", \"green\", \"blue\", \"cyan\", \"magenta\", \"yellow\",\n",
    "    \"orange\", \"purple\", \"pink\", \"lime\",\n",
    "    \"teal\", \"brown\", \"olive\", \"navy\", \"gold\", \"salmon\",\n",
    "    \"turquoise\", \"indigo\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90aa1e6d-ac6c-4c6b-8d8e-b744b0a80cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pathology_animation(image, heatmaps, interval=100, figsize=(6, 6)):\n",
    "    \"\"\"\n",
    "    Create an animated slice-by-slice overlay of heatmaps with a static legend.\n",
    "\n",
    "    Parameters:\n",
    "        image (ndarray): 3D array (Z, H, W) representing the background image (e.g., CT slices).\n",
    "        heatmaps (dict): Dictionary mapping pathology names to 3D arrays (Z, H, W) of activation maps.\n",
    "        interval (int): Delay between frames in milliseconds.\n",
    "        figsize (tuple): Size of the matplotlib figure.\n",
    "    \n",
    "    Returns:\n",
    "        ani (matplotlib.animation.ArtistAnimation): Animation object.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create color maps\n",
    "    cmaps = {\n",
    "        pathology: LinearSegmentedColormap.from_list(\n",
    "            f\"{pathology.replace(' ', '_')}_cmap\",\n",
    "            [to_rgba(\"black\", 0.0), to_rgba(color, 1.0)]\n",
    "        )\n",
    "        for pathology, color in zip(pathologies, colors)\n",
    "    }\n",
    "\n",
    "    pathology_colors = {\n",
    "        pathology: to_hex(to_rgba(color, 1.0))\n",
    "        for pathology, color in zip(pathologies, colors)\n",
    "    }\n",
    "\n",
    "    # Set up figure\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ims = []\n",
    "\n",
    "    # Create animation frames\n",
    "    for slice_idx in range(image.shape[0]):\n",
    "        im_frame = []\n",
    "\n",
    "        # Base CT slice\n",
    "        im = ax.imshow(image[slice_idx], cmap=\"bone\", animated=True)\n",
    "        im_frame.append(im)\n",
    "\n",
    "        # Overlay each pathology's heatmap and contour\n",
    "        for pathology in heatmaps.keys():\n",
    "            imslice = heatmaps[pathology][slice_idx]\n",
    "            im2 = ax.imshow(imslice, cmap=cmaps[pathology], vmin=0, vmax=1, alpha=imslice, animated=True)\n",
    "            im_frame.append(im2)\n",
    "\n",
    "        ax.axis(\"off\")\n",
    "        ims.append(im_frame)\n",
    "\n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], color=pathology_colors[pathology], lw=2, label=pathology)\n",
    "        for pathology in heatmaps.keys()\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize='small', frameon=True)\n",
    "\n",
    "    # Build animation\n",
    "    ani = animation.ArtistAnimation(fig, ims, interval=interval, blit=False, repeat_delay=1000)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b561cba9-93e9-4f52-885d-424da6c45353",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/conda_container_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded state dictionary from: /project/project_465001111/ct_clip/pretrained_models/ctclip_v2.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CTCLIP(\n",
       "  (text_transformer): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.25, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.25, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.25, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (visual_transformer): CTViT(\n",
       "    (spatial_rel_pos_bias): ContinuousPositionBias(\n",
       "      (net): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): LeakyReLU(negative_slope=0.1)\n",
       "        )\n",
       "        (2): Linear(in_features=512, out_features=8, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (to_patch_emb_first_frame): Sequential(\n",
       "      (0): Rearrange('b c 1 (h p1) (w p2) -> b 1 h w (c p1 p2)', p1=20, p2=20)\n",
       "      (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=400, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (to_patch_emb): Sequential(\n",
       "      (0): Rearrange('b c (t pt) (h p1) (w p2) -> b t h w (c pt p1 p2)', p1=20, p2=20, pt=10)\n",
       "      (1): LayerNorm((4000,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=4000, out_features=512, bias=True)\n",
       "      (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (enc_spatial_transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): PEG(\n",
       "            (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "            (context_norm): LayerNorm()\n",
       "            (to_q): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "          )\n",
       "          (2): None\n",
       "          (3): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm()\n",
       "    )\n",
       "    (enc_temporal_transformer): Transformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): PEG(\n",
       "            (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "          )\n",
       "          (1): Attention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (norm): LayerNorm()\n",
       "            (context_norm): LayerNorm()\n",
       "            (to_q): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (to_kv): Linear(in_features=512, out_features=512, bias=False)\n",
       "            (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "          )\n",
       "          (2): None\n",
       "          (3): Sequential(\n",
       "            (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "            (2): GEGLU()\n",
       "            (3): Dropout(p=0.0, inplace=False)\n",
       "            (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_out): LayerNorm()\n",
       "    )\n",
       "    (vq): VectorQuantize(\n",
       "      (project_in): Identity()\n",
       "      (project_out): Identity()\n",
       "      (_codebook): CosineSimCodebook()\n",
       "    )\n",
       "  )\n",
       "  (to_text_latent): Linear(in_features=768, out_features=512, bias=False)\n",
       "  (to_visual_latent): Linear(in_features=294912, out_features=512, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized', do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\").to(device)\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dim_latent = 512\n",
    "dim_text = 768\n",
    "vit_dim_image = 294912\n",
    "\n",
    "vit_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "model = CTCLIP(\n",
    "    text_encoder = text_encoder,\n",
    "    image_encoder = vit_encoder,\n",
    "    dim_text = dim_text,\n",
    "    dim_image = vit_dim_image,\n",
    "    dim_latent = dim_latent\n",
    ")\n",
    "\n",
    "model.load(\"/project/project_465001111/ct_clip/pretrained_models/ctclip_v2.pt\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78215303-c5ca-49de-ae80-e4df1bed0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = \"/scratch/project_465001111/ct_clip/data_volumes/dataset/valid\"\n",
    "valid_reports = \"/project/project_465001111/ct_clip/CT-CLIP-UT/reports/valid_reports.csv\"\n",
    "valid_labels = \"/project/project_465001111/ct_clip/CT-CLIP-UT/labels/valid_labels.csv\"\n",
    "valid_metadata = \"/project/project_465001111/ct_clip/CT-CLIP-UT/metadata/valid_metadata.csv\"\n",
    "\n",
    "ds = InferenceDataset(data_folder=data_valid, reports=valid_reports, metadata=valid_metadata, labels=valid_labels, num_samples=4)\n",
    "sampler = RandomSampler(ds)\n",
    "dl = DataLoader(ds, batch_size=1, sampler=sampler, num_workers=4)\n",
    "arithmetic_embeds = np.load(\"/project/project_465001111/ct_clip/CT-CLIP-UT/src/resources/pathology_diff_embeddings.npy\", allow_pickle=True)\n",
    "arithmetic_embeds = arithmetic_embeds.item()\n",
    "tensor_embeds = {k: torch.tensor(v, dtype=torch.float32).to(device) for k, v in arithmetic_embeds.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d4ec0e-6b1e-432e-9122-837c40f9d98e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on: Lung opacity\n",
      "total patches 12167\n",
      "Patch 1/12167 (0.01%) - Elapsed: 0.1s - ETA: 1197.4s - Patch Time: 0.09840655326843262\n",
      "Patch 101/12167 (0.83%) - Elapsed: 9.7s - ETA: 1158.7s - Patch Time: 0.09491276741027832\n",
      "Patch 201/12167 (1.65%) - Elapsed: 19.3s - ETA: 1148.7s - Patch Time: 0.09596872329711914\n",
      "Patch 301/12167 (2.47%) - Elapsed: 28.9s - ETA: 1138.4s - Patch Time: 0.0966348648071289\n",
      "Patch 401/12167 (3.30%) - Elapsed: 38.5s - ETA: 1128.8s - Patch Time: 0.09547209739685059\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(dl):\n",
    "    image, text, labels, scan_name, original_scan_path = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "    \n",
    "    positive_indices = (labels[0] == 1).nonzero(as_tuple=True)[0]\n",
    "    positive_pathologies = [pathologies[i] for i in positive_indices.tolist()]\n",
    "    heatmaps = {}\n",
    "    heatmaps_threshold = {}\n",
    "    \n",
    "    for pos_pathology in positive_pathologies:\n",
    "        print(\"Working on:\", pos_pathology)\n",
    "        text_embeds = tensor_embeds[pos_pathology]\n",
    "    \n",
    "        _, _, D, H, W = image.shape\n",
    "        heatmap = np.zeros((D, H, W))\n",
    "        patch_size=(20,40,40)\n",
    "        stride=(10,20,20)\n",
    "    \n",
    "        d_coords = range(0, D - patch_size[0] + 1, stride[0])\n",
    "        h_coords = range(0, H - patch_size[1] + 1, stride[1])\n",
    "        w_coords = range(0, W - patch_size[2] + 1, stride[2])\n",
    "    \n",
    "        patch_coords = [\n",
    "            (d, h, w)\n",
    "            for d in d_coords\n",
    "            for h in h_coords\n",
    "            for w in w_coords\n",
    "        ]\n",
    "        total_patches = len(patch_coords)\n",
    "        print(\"total patches\", total_patches)\n",
    "        patch_times = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            regular_sim_matrix, *_ = model(None, image, text_embeds)\n",
    "            regular_score = regular_sim_matrix.item()\n",
    "            start_time = time.time()\n",
    "    \n",
    "            for idx, (d, h, w) in enumerate(patch_coords):\n",
    "                patch_start = time.time()\n",
    "                occluded_image = image.clone().detach()\n",
    "                occluded_image[:, :, d:d+patch_size[0], h:h+patch_size[1], w:w+patch_size[2]] = -1\n",
    "                \n",
    "                occluded_sim_matrix, *_ = model(None, occluded_image, text_embeds)\n",
    "                occluded_score = occluded_sim_matrix.item()\n",
    "    \n",
    "                importance = max(regular_score - occluded_score, 0)\n",
    "                heatmap[d:d+patch_size[0], h:h+patch_size[1], w:w+patch_size[2]] += importance\n",
    "    \n",
    "                if idx % 100 == 0 or idx == total_patches - 1:\n",
    "                    patch_time = time.time() - patch_start\n",
    "                    patch_times.append(patch_time)\n",
    "                    elapsed = time.time() - start_time\n",
    "                    avg_time_per_patch = elapsed / (idx + 1)\n",
    "                    remaining_time = avg_time_per_patch * (total_patches - (idx + 1))\n",
    "                    percent_done = 100.0 * (idx + 1) / total_patches\n",
    "    \n",
    "                    print(f\"Patch {idx + 1}/{total_patches} \"\n",
    "                        f\"({percent_done:.2f}%) - Elapsed: {elapsed:.1f}s - ETA: {remaining_time:.1f}s - Patch Time: {patch_time}\")\n",
    "\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "        heatmap = np.rot90(heatmap, k=-1, axes=(1,2))\n",
    "        \n",
    "        heatmap_threshold = heatmap.copy()\n",
    "        heatmap_threshold[heatmap_threshold < 0.5] = 0\n",
    "        \n",
    "        heatmaps[pos_pathology] = heatmap\n",
    "        heatmaps_threshold[pos_pathology] = heatmap_threshold\n",
    "\n",
    "    print(np.mean(patch_times))\n",
    "    image = image.squeeze().cpu().numpy()\n",
    "    image = np.rot90(image, k=-1, axes=(1, 2))\n",
    "    ani = create_pathology_animation(\n",
    "        image=image,\n",
    "        heatmaps=heatmaps,\n",
    "        interval=100\n",
    "    )\n",
    "    ani_threshold = create_pathology_animation(\n",
    "        image=image,\n",
    "        heatmaps=heatmaps_threshold,\n",
    "        interval=100\n",
    "    )\n",
    "    ani.save(f\"{scan_name[0]}_occlusion_204040_102020_nothreshold.gif\", writer=\"pillow\", fps=10)\n",
    "    ani_threshold.save(f\"{scan_name[0]}_occlusion_204040_102020_threshold05.gif\", writer=\"pillow\", fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc55d044-5a06-4721-b5c6-29efe983d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_rollout_3d(attn_list):\n",
    "    \"\"\"\n",
    "    Applies attention rollout over spatial tokens for each temporal slice.\n",
    "    \n",
    "    Args:\n",
    "        attn_list: list of 4 tensors, each [24, 8, 576, 576]\n",
    "\n",
    "    Returns:\n",
    "        rollout_list: list of 24 tensors, each [576, 576]\n",
    "    \"\"\"\n",
    "    num_layers = len(attn_list)\n",
    "    num_temporal = attn_list[0].shape[0]\n",
    "    num_heads = attn_list[0].shape[1]\n",
    "    num_tokens = attn_list[0].shape[2]\n",
    "\n",
    "    rollout_list = []\n",
    "\n",
    "    for t in range(num_temporal):\n",
    "        # Extract per-slice attention stack\n",
    "        slice_stack = [attn_list[l][t] for l in range(num_layers)]  # each is [8, 576, 576]\n",
    "\n",
    "        # Step 1: average over heads → list of [576, 576]\n",
    "        attn_avg = [A.mean(dim=0) for A in slice_stack]\n",
    "\n",
    "        # Step 2: add identity, normalize rows\n",
    "        attn_aug = []\n",
    "        for A in attn_avg:\n",
    "            A = 0.5 * A + 0.5 * torch.eye(A.size(0), device=A.device)\n",
    "            A = A / A.sum(dim=-1, keepdim=True)\n",
    "            attn_aug.append(A)\n",
    "\n",
    "        # Step 3: rollout\n",
    "        rollout = attn_aug[0]\n",
    "        for A in attn_aug[1:]:\n",
    "            rollout = torch.matmul(A, rollout)\n",
    "\n",
    "        rollout_list.append(rollout)  # [576, 576] per temporal slice\n",
    "\n",
    "    return rollout_list  # list of 24 [576, 576] matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "935a2f1d-6d5f-47c9-bcab-89174e469dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([24, 8, 576, 576])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n",
      "self_weights shape torch.Size([576, 8, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "for batch in iter(dl):\n",
    "    image, text, labels, scan_name, original_scan_path = [b.to(device) if isinstance(b, torch.Tensor) else b for b in batch]\n",
    "\n",
    "    text_tokens = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        *_, spatial_attention_weights, temporal_attention_weights = model(text_tokens, image)\n",
    "        start_time = time.time()\n",
    "\n",
    "        \n",
    "    image = image.squeeze().cpu().numpy()\n",
    "    image = np.rot90(image, k=-1, axes=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38000e-5cdc-41b3-941e-e2df078e2dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
